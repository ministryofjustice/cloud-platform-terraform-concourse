resources:
  - name: cloud-platform-cli-image
    type: registry-image
    source:
      repository: ministryofjustice/cloud-platform-cli
      tag: "1.39.5"
      username: ((ministryofjustice-dockerhub.dockerhub_username))
      password: ((ministryofjustice-dockerhub.dockerhub_password))

kube-config: &KUBECONFIG_PARAMS
  KUBECONFIG: /tmp/kubeconfig
  KUBECONFIG_CLUSTER_NAME: ((cluster_name))

aws_params: &aws_params
  AWS_ACCESS_KEY_ID: ((aws-creds.access-key-id))
  AWS_SECRET_ACCESS_KEY: ((aws-creds.secret-access-key))
  AWS_REGION: eu-west-2
  AWS_PROFILE: moj-cp

groups:
  - name: cordon-and-drain-nodes
    jobs:
      - cordon-and-drain-nodes

jobs:
  - name: cordon-and-drain-nodes
    max_in_flight: 1
    plan:
      - get: cloud-platform-cli-image
      - task: cordon-and-drain-nodes
        timeout: 48h
        image: cloud-platform-cli-image
        config:
          platform: linux
          params:
            <<: [*aws_params, *KUBECONFIG_PARAMS]
            NODE_GROUP_TO_DRAIN: ((node_group_to_drain))
          run:
            path: /bin/bash
            args:
              - -c
              - |
                set -e

                mkdir ${HOME}/.aws
                echo "[moj-cp]" >> ${HOME}/.aws/credentials # This forces you to have profiles

                drain_and_remove_node() {
                  echo "starting node recycle for: $1"

                  kubectl get node $1
                  if  [[ $? -eq 1 ]]; then
                    echo "Node $1 not found, it's probably already been removed by the autoscaler, skipping cordon and drain"
                    return
                  fi

                  kubectl get pods --field-selector="status.phase=Failed,spec.nodeName=$1" -A --no-headers | awk '{print $2 " -n " $1}' | parallel -j1 --will-cite kubectl delete pod "{= uq =}"

                  cloud-platform cluster recycle-node --name $1 --skip-version-check --kubecfg $KUBECONFIG --drain-only --ignore-label --force

                  sleep 270

                  NODES_IN_NG=$(kubectl get nodes -l eks.amazonaws.com/nodegroup=$NODE_GROUP_TO_DRAIN --no-headers | wc -l)

                  echo "number of nodes left in the node group: $NODES_IN_NG"

                  if (( $NODES_IN_NG > 1 )); then
                    echo "removing node $1 from the node group"

                    INSTANCE_ID=$(kubectl get node $1 -ojson | jq -r '.metadata.annotations["csi.volume.kubernetes.io/nodeid"] | fromjson | .["ebs.csi.aws.com"]')

                    kubectl delete node $1

                    aws autoscaling terminate-instance-in-auto-scaling-group --instance-id $INSTANCE_ID --no-should-decrement-desired-capacity --region eu-west-2

                    echo "finished draining node and have removed node from ASG"
                  fi
                }

                disable_autoscaling() {
                  # disable auto scaling in the node group
                  ASG_NAME=$(aws eks --region eu-west-2 describe-nodegroup --cluster-name $KUBECONFIG_CLUSTER_NAME --nodegroup-name $NODE_GROUP_TO_DRAIN  | jq -r ".nodegroup.resources.autoScalingGroups[0].name")
                  aws autoscaling suspend-processes --auto-scaling-group-name $ASG_NAME

                  # needed to allow the node group to be deleted by terraform later
                  aws autoscaling resume-processes --auto-scaling-group-name $ASG_NAME --scaling-process Terminate

                  aws autoscaling create-or-update-tags --tags ResourceId=$ASG_NAME,ResourceType=auto-scaling-group,Key=k8s.io/cluster-autoscaler/enabled,Value=false,PropagateAtLaunch=true
                }

                clean_stuck_pods() {
                  delete_pods() {
                        NAMESPACE=$(echo "$1" | sed -E 's/\/api\/v1\/namespaces\/(.*)\/pods\/.*/\1/')
                        POD=$(echo "$1" | sed -E 's/.*\/pods\/(.*)\/eviction.*/\1/')
                        echo $NAMESPACE
                        echo $POD
                        kubectl delete pod -n $NAMESPACE $POD
                      }
                    export -f delete_pods

                    while (( `kubectl get nodes -l eks.amazonaws.com/nodegroup=$NODE_GROUP_TO_DRAIN --no-headers | wc -l` > 1 ))
                      do
                          TIME_NOW_EPOCH=$(date +%s)
                          START_TIME=$(($TIME_NOW_EPOCH - 180))
                          CLUSTER_LOG_GROUP="/aws/eks/$KUBECONFIG_CLUSTER_NAME/cluster"
                          QUERY_ID=$(aws logs start-query \
                            --start-time $START_TIME \
                            --end-time $TIME_NOW_EPOCH \
                            --log-group-name $CLUSTER_LOG_GROUP \
                            --query-string 'fields @timestamp, @message | filter @logStream like "kube-apiserver-audit" | filter ispresent(requestURI) | filter objectRef.subresource = "eviction" | filter responseObject.status = "Failure" | display @logStream, requestURI, responseObject.message | stats count(*) as retry by requestURI, requestObject.message' \
                            | jq -r '.queryId' )
                          sleep 60
                          RESULTS=$(aws logs get-query-results --query-id $QUERY_ID)
                          echo -n $RESULTS | jq '.results[]' | grep '/api/v1' | awk '{ print $2 }' | xargs -I {} bash -c 'delete_pods {}'
                      done
                }

                aws eks --region eu-west-2 update-kubeconfig --name $KUBECONFIG_CLUSTER_NAME

                disable_autoscaling

                clean_stuck_pods &

                export -f drain_and_remove_node
                kubectl get nodes -l eks.amazonaws.com/nodegroup=$NODE_GROUP_TO_DRAIN --sort-by=metadata.creationTimestamp --no-headers | awk '{print $1}' | parallel -j1 --keep-order --delay 300 --will-cite drain_and_remove_node $1

